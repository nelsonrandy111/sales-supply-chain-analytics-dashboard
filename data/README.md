# Data Directory

This directory contains the data files used by the Sales & Supply Chain Analytics Dashboard.

## Required Data Files

### Main Dataset
- **`DataCoSupplyChainDataset.csv`** (91MB)
  - Raw supply chain transaction data
  - Contains order details, customer information, product data
  - Includes geographic coordinates for mapping
  - Covers multiple years of transaction data
  - **Source**: [DataCo Smart Supply Chain for Big Data Analysis](https://www.kaggle.com/datasets/shashwatwork/dataco-smart-supply-chain-for-big-data-analysis) on Kaggle
  - **Original Author**: Shashwat Work
  - **License**: CC0: Public Domain

### Processed Data
- **`clustering_data.csv`** (1.6MB)
  - Preprocessed data for customer clustering
  - Contains customer-level features and RFM metrics
  - Generated by `clustering_cleaning.R`

## Data Structure

### DataCoSupplyChainDataset.csv Columns
- **Order Information**: Order ID, dates, status, shipping details
- **Customer Information**: Customer ID, name, location, segment
- **Product Information**: Product name, category, department, price
- **Financial Data**: Sales, profit, discount rates
- **Geographic Data**: Latitude, longitude, city, state, country
- **Shipping Data**: Delivery status, shipping mode, delivery times

### clustering_data.csv Columns
- **Customer Metrics**: Total orders, sales, profit, discount
- **RFM Features**: Recency, frequency, monetary values
- **Behavioral Features**: Average order values, shipping errors
- **Segment Indicators**: Consumer vs business customer flags

## Data Processing

### Preprocessing Steps
1. **Data Cleaning**: Remove duplicates, handle missing values
2. **Feature Engineering**: Create RFM metrics and behavioral features
3. **Customer Aggregation**: Summarize at customer level
4. **Scaling**: Normalize features for clustering algorithms

### Clustering Features
- **Recency**: Days since last purchase
- **Frequency**: Total number of orders
- **Monetary**: Total sales and profit
- **Behavioral**: Average order values, discount patterns
- **Geographic**: Shipping patterns and delivery performance

## Data Requirements

### For Dashboard Functionality
- Minimum 1,000 customer records
- At least 6 months of transaction history
- Geographic coordinates for mapping features
- Customer segment information

### For Clustering Analysis
- Complete customer transaction history
- Consistent date formats
- Valid numeric values for financial metrics
- Geographic data for location-based analysis

## Data Privacy

### Sensitive Information
- Customer names and contact information should be anonymized
- Email addresses and personal identifiers should be removed
- Consider data masking for production environments

### Data Handling
- Store sensitive data securely
- Use environment variables for database connections
- Implement proper access controls
- Follow data protection regulations

## Data Validation

### Quality Checks
- Verify date ranges and formats
- Check for duplicate records
- Validate geographic coordinates
- Ensure numeric fields contain valid values
- Confirm customer ID consistency

### Common Issues
- Missing geographic coordinates
- Inconsistent date formats
- Duplicate customer records
- Invalid financial calculations
- Outdated customer segments

## Getting Started

### For New Users
1. Place `DataCoSupplyChainDataset.csv` in the `data/` directory
2. Run `clustering_cleaning.R` to generate processed data
3. Verify `clustering_data.csv` is created successfully in the `data/` directory
4. Test the dashboard with `shiny::runApp()`

### For Custom Data
1. Ensure your data matches the expected column structure
2. Update `clustering_cleaning.R` for custom features
3. Modify clustering parameters as needed
4. Test with sample data before full deployment

## Sample Data

For testing purposes, consider creating a smaller sample dataset:
- 1,000-5,000 customer records
- 6-12 months of transaction history
- Representative geographic distribution
- Balanced customer segments

## Troubleshooting

### Common Data Issues
- **Missing Coordinates**: Update mapping features or use alternative location data
- **Date Format Errors**: Check date parsing in preprocessing scripts
- **Memory Issues**: Consider data sampling for large datasets
- **Clustering Errors**: Verify feature scaling and outlier detection

### Performance Optimization
- Use data.table for large datasets
- Implement data caching for Shiny app
- Consider database storage for production
- Optimize clustering algorithms for speed

---

**Note**: Always backup your original data files before processing or modification. 